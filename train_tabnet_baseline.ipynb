{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39VT36i8rCPH"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# [1] Required package installation\n",
        "# ==========================================\n",
        "# 1. pytorch-tabnet: core implementation for the TabNet baseline\n",
        "# 2. pytorch-widedeep: included for compatibility with related deep tabular baselines\n",
        "# 3. optuna: hyperparameter optimization\n",
        "!pip install pytorch-tabnet pytorch-widedeep optuna\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import io\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# TabNet (original implementation)\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "# Deep tabular baseline utilities from pytorch-widedeep\n",
        "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
        "from pytorch_widedeep.models import SAINT, FTTransformer\n",
        "from pytorch_widedeep import Trainer\n",
        "from pytorch_widedeep.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [1] Data loading and preprocessing\n",
        "#     (standard TabNet baseline setting)\n",
        "# ==========================================\n",
        "def load_and_prep():\n",
        "    # 1. Load dataset\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"=== [Google Colab] Please upload Dataset.csv ===\")\n",
        "        uploaded = files.upload()\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "    except:\n",
        "        print(\"=== [Local Environment] Loading Dataset.csv ===\")\n",
        "        df = pd.read_csv('Dataset.csv')\n",
        "\n",
        "    # 2. Basic preprocessing\n",
        "    if 'Country Name' in df.columns:\n",
        "        df = df.drop('Country Name', axis=1)\n",
        "\n",
        "    cat_cols = ['Country Code', 'Continent']\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "    target = 'Maternal Mortality Ratio'\n",
        "    exclude = cat_cols + ['Year', target]\n",
        "    num_cols = [c for c in df.columns if c not in exclude]\n",
        "\n",
        "    # Numerical missing values -> median imputation\n",
        "    for col in num_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # 3. Encoding and split setup\n",
        "    categorical_dims = {}\n",
        "    for col in cat_cols:\n",
        "        l_enc = LabelEncoder()\n",
        "        df[col] = l_enc.fit_transform(df[col].values)\n",
        "        categorical_dims[col] = len(l_enc.classes_)\n",
        "\n",
        "    features = [c for c in df.columns if c not in ['Year', target]]\n",
        "    cat_idxs = [i for i, f in enumerate(features) if f in cat_cols]\n",
        "    cat_dims = [categorical_dims[f] for f in features if f in cat_cols]\n",
        "\n",
        "    # Temporal data splits\n",
        "    # Phase 1: model selection (Train 2011-2014 / Validation 2015)\n",
        "    train_p1 = df[(df['Year'] >= 2011) & (df['Year'] <= 2014)]\n",
        "    val_p1   = df[df['Year'] == 2015]\n",
        "\n",
        "    # Phase 2: final retraining on the pretest period (2011-2015)\n",
        "    train_p2 = df[(df['Year'] >= 2011) & (df['Year'] <= 2015)]\n",
        "\n",
        "    # Held-out test year\n",
        "    test = df[df['Year'] == 2016]\n",
        "\n",
        "    def to_xy(data):\n",
        "        return data[features].values, data[target].values.reshape(-1, 1)\n",
        "\n",
        "    return (to_xy(train_p1), to_xy(val_p1), to_xy(train_p2), to_xy(test),\n",
        "            cat_idxs, cat_dims, features)\n",
        "\n",
        "# Prepare data\n",
        "(data_train_1, data_val_1, data_train_2, data_test,\n",
        " cat_idxs, cat_dims, feature_names) = load_and_prep()\n",
        "\n",
        "X_train_1, y_train_1 = data_train_1\n",
        "X_val_1, y_val_1     = data_val_1\n",
        "X_train_2, y_train_2 = data_train_2\n",
        "X_test, y_test       = data_test"
      ],
      "metadata": {
        "id": "sQwNtBPWrIYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [2] Optuna-based hyperparameter optimization\n",
        "# ==========================================\n",
        "def objective(trial):\n",
        "    # Search space definition\n",
        "    param = {\n",
        "        'n_d': trial.suggest_int('n_d', 8, 64, step=8),          # width of decision layers\n",
        "        'n_a': trial.suggest_int('n_a', 8, 64, step=8),          # width of attention layers\n",
        "        'n_steps': trial.suggest_int('n_steps', 3, 10),          # number of sequential decision steps\n",
        "        'gamma': trial.suggest_float('gamma', 1.0, 2.0),         # relaxation parameter\n",
        "        'n_independent': trial.suggest_int('n_independent', 1, 5),\n",
        "        'n_shared': trial.suggest_int('n_shared', 1, 5),\n",
        "        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-4, 1e-2, log=True),\n",
        "        'optimizer_params': {'lr': trial.suggest_float('lr', 1e-3, 1e-1, log=True)},\n",
        "    }\n",
        "\n",
        "    # Baseline TabNet model\n",
        "    clf = TabNetRegressor(\n",
        "        cat_idxs=cat_idxs,\n",
        "        cat_dims=cat_dims,\n",
        "        cat_emb_dim=1,\n",
        "        scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        mask_type='entmax',\n",
        "        verbose=0,\n",
        "        **param\n",
        "    )\n",
        "\n",
        "    # Train on the model-selection split\n",
        "    clf.fit(\n",
        "        X_train=X_train_1, y_train=y_train_1,\n",
        "        eval_set=[(X_val_1, y_val_1)],\n",
        "        eval_name=['valid'],\n",
        "        eval_metric=['mae'],\n",
        "        max_epochs=100,\n",
        "        patience=15,\n",
        "        batch_size=256,\n",
        "        virtual_batch_size=128,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    # Return the best validation score\n",
        "    return clf.best_cost\n",
        "\n",
        "print(\"\\n>>> [Step 1] Starting Optuna hyperparameter optimization (20 trials)...\")\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"\\n>>> [Result] Best hyperparameters\")\n",
        "best_params = study.best_trial.params\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "YbiDkcPqrKXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [3] Retraining on the full pretest period\n",
        "# ==========================================\n",
        "print(\"\\n>>> [Step 2] Retraining the optimized TabNet baseline on 2011-2015...\")\n",
        "\n",
        "# Parse optimized parameters\n",
        "final_params = best_params.copy()\n",
        "lr = final_params.pop('lr')\n",
        "\n",
        "clf_final = TabNetRegressor(\n",
        "    cat_idxs=cat_idxs,\n",
        "    cat_dims=cat_dims,\n",
        "    cat_emb_dim=1,\n",
        "    optimizer_params={'lr': lr},\n",
        "    scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type='entmax',\n",
        "    verbose=10,\n",
        "    **final_params\n",
        ")\n",
        "\n",
        "clf_final.fit(\n",
        "    X_train=X_train_2, y_train=y_train_2,\n",
        "    eval_set=None,\n",
        "    max_epochs=150,\n",
        "    batch_size=256,\n",
        "    virtual_batch_size=128,\n",
        "    drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "1ORQ9GYUrMH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [4] Final evaluation and visualization\n",
        "# ==========================================\n",
        "preds = clf_final.predict(X_test)\n",
        "\n",
        "print(\"\\n=== [Step 3] Held-out Test Results (Year 2016) ===\")\n",
        "print(f\"MAE : {mean_absolute_error(y_test, preds):.4f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, preds)):.4f}\")\n",
        "print(f\"R2  : {r2_score(y_test, preds):.4f}\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Feature importance\n",
        "# ------------------------------------------\n",
        "plt.figure(figsize=(10, 5))\n",
        "importances = pd.Series(clf_final.feature_importances_, index=feature_names)\n",
        "importances.nlargest(10).sort_values().plot(kind='barh', color='#6c5ce7')\n",
        "plt.title('TabNet Baseline Feature Importance')\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Attention masks\n",
        "# ------------------------------------------\n",
        "explain_matrix, masks = clf_final.explain(X_test)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for i in range(3):\n",
        "    sns.heatmap(masks[i][:15], ax=axs[i], cbar=False, cmap='viridis')\n",
        "    axs[i].set_title(f\"Decision-Step Mask {i} (Top 15 Samples)\")\n",
        "    axs[i].set_xlabel(\"Features\")\n",
        "    axs[i].set_ylabel(\"Samples\")\n",
        "\n",
        "plt.suptitle(\"TabNet Baseline Attention Masks\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D8a5j69WrO_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
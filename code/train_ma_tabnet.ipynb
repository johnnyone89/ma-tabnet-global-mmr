{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvafhb1il9k5"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# [1] Required package installation\n",
        "# ==========================================\n",
        "# 1. pytorch-tabnet: core implementation for TabNet\n",
        "# 2. pytorch-widedeep: useful for SAINT / FT-Transformer experiments\n",
        "# 3. optuna: hyperparameter optimization\n",
        "!pip install pytorch-tabnet pytorch-widedeep optuna\n",
        "\n",
        "%pip install pytorch_tabnet\n",
        "\n",
        "# ==========================================\n",
        "# Experiment 2: MA-TabNet\n",
        "# ==========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import io\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# TabNet\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [1] Data loading and preprocessing\n",
        "#     (enhanced MA-TabNet version)\n",
        "# ==========================================\n",
        "def load_and_prep_ma_tabnet_advanced():\n",
        "    # 1. Load dataset\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"=== [Google Colab] Please upload Dataset.csv ===\")\n",
        "        uploaded = files.upload()\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "    except:\n",
        "        print(\"=== [Local Environment] Loading Dataset.csv ===\")\n",
        "        df = pd.read_csv('Dataset.csv')\n",
        "\n",
        "    # 2. Basic preprocessing\n",
        "    if 'Country Name' in df.columns:\n",
        "        df = df.drop('Country Name', axis=1)\n",
        "\n",
        "    target = 'Maternal Mortality Ratio'\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # [Key MA-TabNet design] Selective masking strategy\n",
        "    # Instead of generating missing-value masks for all variables,\n",
        "    # we only create masks for features whose missingness exceeds\n",
        "    # a predefined threshold. The goal is to preserve informative\n",
        "    # missingness patterns while avoiding unnecessary indicator noise.\n",
        "    # ---------------------------------------------------------\n",
        "    MISSING_THRESHOLD = 0.05  # generate masks only for variables with >= 5% missingness\n",
        "\n",
        "    cat_cols = ['Country Code', 'Continent']\n",
        "    exclude = cat_cols + ['Year', target]\n",
        "    num_cols = [c for c in df.columns if c not in exclude]\n",
        "\n",
        "    added_masks = []\n",
        "\n",
        "    for col in num_cols:\n",
        "        missing_ratio = df[col].isnull().mean()\n",
        "\n",
        "        # Only features with nontrivial missingness are treated as\n",
        "        # carrying meaningful observation-pattern information.\n",
        "        if missing_ratio >= MISSING_THRESHOLD:\n",
        "            mask_col_name = f\"{col}_is_missing\"\n",
        "\n",
        "            # IMPORTANT:\n",
        "            # The mask is treated as a numerical feature (0.0 / 1.0),\n",
        "            # not as a categorical feature. This keeps the architecture\n",
        "            # simple and allows TabNet to exploit the signal efficiently.\n",
        "            df[mask_col_name] = df[col].isnull().astype(float)\n",
        "            added_masks.append(mask_col_name)\n",
        "\n",
        "    # Impute numerical missing values with the feature-wise median\n",
        "    for col in num_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # Impute categorical missing values with an explicit \"Unknown\" category\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "    # 3. Categorical encoding\n",
        "    categorical_dims = {}\n",
        "    for col in cat_cols:\n",
        "        l_enc = LabelEncoder()\n",
        "        df[col] = l_enc.fit_transform(df[col].astype(str).values)\n",
        "        categorical_dims[col] = len(l_enc.classes_)\n",
        "\n",
        "    # Reconstruct feature list:\n",
        "    # [original numerical features] + [selected missingness masks as numeric]\n",
        "    # + [categorical features]\n",
        "    features = [c for c in df.columns if c not in ['Year', target]]\n",
        "\n",
        "    # Categorical indices used for embedding layers\n",
        "    # (only true categorical variables, not missingness masks)\n",
        "    cat_idxs = [i for i, f in enumerate(features) if f in cat_cols]\n",
        "    cat_dims = [categorical_dims[f] for f in features if f in cat_cols]\n",
        "\n",
        "    print(f\"\\n[Model Info] Total number of input features: {len(features)}\")\n",
        "    print(f\"[Model Info] Number of selectively added missingness masks: {len(added_masks)}\")\n",
        "    print(f\" -> Strategy: only variables with missingness >= {MISSING_THRESHOLD*100:.1f}% are augmented with mask indicators\")\n",
        "\n",
        "    # Temporal data splits\n",
        "    def to_xy(data):\n",
        "        return data[features].values, data[target].values.reshape(-1, 1)\n",
        "\n",
        "    train_p1 = df[(df['Year'] >= 2011) & (df['Year'] <= 2014)]\n",
        "    val_p1   = df[df['Year'] == 2015]\n",
        "    train_p2 = df[(df['Year'] >= 2011) & (df['Year'] <= 2015)]\n",
        "    test     = df[df['Year'] == 2016]\n",
        "\n",
        "    return (to_xy(train_p1), to_xy(val_p1), to_xy(train_p2), to_xy(test),\n",
        "            cat_idxs, cat_dims, features, added_masks)\n",
        "\n",
        "# Prepare data\n",
        "(data_train_1, data_val_1, data_train_2, data_test,\n",
        " cat_idxs, cat_dims, feature_names, added_masks) = load_and_prep_ma_tabnet_advanced()\n",
        "\n",
        "X_train_1, y_train_1 = data_train_1\n",
        "X_val_1, y_val_1     = data_val_1\n",
        "X_train_2, y_train_2 = data_train_2\n",
        "X_test, y_test       = data_test"
      ],
      "metadata": {
        "id": "C494oMDynCGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [2] Optuna-based hyperparameter search\n",
        "# ==========================================\n",
        "def objective(trial):\n",
        "    # Because the feature space expands after selective masking,\n",
        "    # the search space allows slightly larger model capacity.\n",
        "    param = {\n",
        "        'n_d': trial.suggest_int('n_d', 16, 64, step=8),\n",
        "        'n_a': trial.suggest_int('n_a', 16, 64, step=8),\n",
        "        'n_steps': trial.suggest_int('n_steps', 3, 8),\n",
        "        'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
        "        'n_independent': trial.suggest_int('n_independent', 2, 5),\n",
        "        'n_shared': trial.suggest_int('n_shared', 2, 5),\n",
        "        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-4, 1e-2, log=True),\n",
        "        'optimizer_params': {'lr': trial.suggest_float('lr', 1e-2, 5e-2)},\n",
        "    }\n",
        "\n",
        "    clf = TabNetRegressor(\n",
        "        cat_idxs=cat_idxs,\n",
        "        cat_dims=cat_dims,\n",
        "        cat_emb_dim=1,  # only categorical identifiers are embedded\n",
        "        scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        mask_type='entmax',\n",
        "        verbose=0,\n",
        "        **param\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_train=X_train_1, y_train=y_train_1,\n",
        "        eval_set=[(X_val_1, y_val_1)],\n",
        "        eval_name=['valid'],\n",
        "        eval_metric=['mae'],\n",
        "        max_epochs=150,\n",
        "        patience=20,\n",
        "        batch_size=256,\n",
        "        virtual_batch_size=128,\n",
        "        drop_last=False\n",
        "    )\n",
        "    return clf.best_cost\n",
        "\n",
        "print(\"\\n>>> [Step 1] Starting MA-TabNet advanced hyperparameter optimization...\")\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"\\n>>> [Result] Best hyperparameters\")\n",
        "best_params = study.best_trial.params\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "SuDR4oUtnE-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [3] Final training with optimized hyperparameters\n",
        "# ==========================================\n",
        "print(\"\\n>>> [Step 2] Training final MA-TabNet model using the best configuration...\")\n",
        "\n",
        "final_params = best_params.copy()\n",
        "lr = final_params.pop('lr')\n",
        "\n",
        "clf_final = TabNetRegressor(\n",
        "    cat_idxs=cat_idxs,\n",
        "    cat_dims=cat_dims,\n",
        "    cat_emb_dim=1,\n",
        "    optimizer_params={'lr': lr},\n",
        "    scheduler_params={\"step_size\": 10, \"gamma\": 0.95},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type='entmax',\n",
        "    verbose=10,\n",
        "    **final_params\n",
        ")\n",
        "\n",
        "clf_final.fit(\n",
        "    X_train=X_train_2, y_train=y_train_2,\n",
        "    eval_set=None,\n",
        "    max_epochs=200,\n",
        "    batch_size=256,\n",
        "    virtual_batch_size=128,\n",
        "    drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "WSed0OOrnJLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [4] Final evaluation and performance summary\n",
        "# ==========================================\n",
        "preds = clf_final.predict(X_test)\n",
        "print(\"\\n=== [MA-TabNet Advanced Performance on the 2016 Held-out Test Set] ===\")\n",
        "print(f\"MAE : {mean_absolute_error(y_test, preds):.4f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, preds)):.4f}\")\n",
        "print(f\"R2  : {r2_score(y_test, preds):.4f}\")\n",
        "\n",
        "# Inspect feature importance and highlight missingness-mask variables\n",
        "plt.figure(figsize=(10, 6))\n",
        "importances = pd.Series(clf_final.feature_importances_, index=feature_names)\n",
        "\n",
        "colors = ['red' if '_is_missing' in name else '#0984e3'\n",
        "          for name in importances.nlargest(15).sort_values().index]\n",
        "\n",
        "importances.nlargest(15).sort_values().plot(kind='barh', color=colors)\n",
        "plt.title('Feature Importance (Red = Missingness Mask)')\n",
        "plt.show()\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OTTX-7uQnK-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Output file name\n",
        "# =========================\n",
        "FNAME = \"feature_importance_missing_mask_top15\"\n",
        "OUT_PNG = f\"{FNAME}.png\"\n",
        "OUT_PDF = f\"{FNAME}.pdf\""
      ],
      "metadata": {
        "id": "Vt_ReaK7nMoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Feature importance plot\n",
        "# =========================\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "importances = pd.Series(clf_final.feature_importances_, index=feature_names)\n",
        "\n",
        "topk = 15\n",
        "top_imp = importances.nlargest(topk).sort_values()  # sorted for better horizontal bar display\n",
        "\n",
        "# Highlight mask variables in red\n",
        "colors = ['#d63031' if '_is_missing' in name else '#0984e3' for name in top_imp.index]\n",
        "\n",
        "ax = top_imp.plot(kind='barh', color=colors, edgecolor='0.25', linewidth=0.6)\n",
        "\n",
        "ax.set_xlabel(\"Importance\", fontsize=11)\n",
        "ax.set_ylabel(\"\")\n",
        "ax.grid(axis='x', linestyle='--', alpha=0.25, linewidth=0.8)\n",
        "\n",
        "# Clean axis appearance\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_alpha(0.5)\n",
        "ax.spines['bottom'].set_alpha(0.5)\n",
        "\n",
        "ax.tick_params(axis='y', labelsize=10)\n",
        "ax.tick_params(axis='x', labelsize=10)\n",
        "\n",
        "# Add numeric labels at the end of each bar\n",
        "xmax = float(top_imp.max())\n",
        "for i, v in enumerate(top_imp.values):\n",
        "    ax.text(v + xmax * 0.01, i, f\"{v:.4f}\", va='center', fontsize=9)\n",
        "\n",
        "# Small explanatory note\n",
        "ax.text(\n",
        "    0.99, 0.02,\n",
        "    \"Red: missingness-mask feature\\nBlue: original feature\",\n",
        "    transform=ax.transAxes,\n",
        "    ha='right', va='bottom',\n",
        "    fontsize=9, alpha=0.85\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save in high resolution\n",
        "plt.savefig(OUT_PNG, dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.savefig(OUT_PDF, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: {OUT_PNG} (dpi=600)\")\n",
        "print(f\"Saved: {OUT_PDF}\")\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    import seaborn as sns\n",
        "except ImportError:\n",
        "    %pip -q install shap seaborn\n",
        "    import shap\n",
        "    import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.inspection import PartialDependenceDisplay"
      ],
      "metadata": {
        "id": "m443Qc5cnPKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# [Visualization settings]\n",
        "# ==============================================================================\n",
        "# 1. Seaborn theme\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\", font_scale=1.1)\n",
        "\n",
        "# 2. Matplotlib global settings\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': (12, 8),\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 600,\n",
        "    'axes.titlesize': 18,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelsize': 15,\n",
        "    'xtick.labelsize': 13,\n",
        "    'ytick.labelsize': 13,\n",
        "    'font.family': 'sans-serif',\n",
        "})\n",
        "\n",
        "# Color settings\n",
        "PALETTE_GLOBAL = \"viridis\"\n",
        "PALETTE_LOCAL = \"mako\"\n",
        "COLOR_BAR = \"#3498db\"\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "OuGjeVZMnTNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. TabNet attention visualization\n",
        "# ==============================================================================\n",
        "def tabnet_attention_xai_hq(model, X, feature_names, top_k=15, sample_idx=0, save_dir=\".\", dpi=600):\n",
        "    \"\"\"\n",
        "    High-quality visualization of TabNet attention explanations.\n",
        "\n",
        "    Outputs:\n",
        "    - global attention ranking\n",
        "    - stepwise attention summaries\n",
        "    - local attention for a selected sample\n",
        "\n",
        "    All plots are saved in both PNG and PDF formats.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    out = model.explain(X)\n",
        "    if isinstance(out, tuple) and len(out) == 2:\n",
        "        M_explain, masks = out\n",
        "    else:\n",
        "        raise ValueError(\"Please verify the return format of model.explain(X).\")\n",
        "\n",
        "    step_keys = list(masks.keys())\n",
        "    step_keys_sorted = sorted(step_keys)\n",
        "\n",
        "    # --- (1) Global attention importance aggregated across steps\n",
        "    step_global = {}\n",
        "    global_sum = np.zeros(len(feature_names))\n",
        "    for k in step_keys_sorted:\n",
        "        step_global[k] = masks[k].mean(axis=0)\n",
        "        global_sum += step_global[k]\n",
        "    global_sum = global_sum / (len(step_keys_sorted) + 1e-12)\n",
        "\n",
        "    df_global = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': global_sum\n",
        "    }).sort_values(by='Importance', ascending=False).head(top_k)\n",
        "\n",
        "    # === Plot 1: Global attention importance\n",
        "    fname_global = f\"tabnet_global_attention_top{top_k}\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=df_global, x='Importance', y='Feature', palette=PALETTE_GLOBAL, edgecolor=\".2\")\n",
        "    plt.xlabel(\"Mean Attention Score\")\n",
        "    plt.ylabel(\"\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(os.path.join(save_dir, f\"{fname_global}.png\"), dpi=dpi, bbox_inches='tight', facecolor=\"white\")\n",
        "    plt.savefig(os.path.join(save_dir, f\"{fname_global}.pdf\"), bbox_inches='tight', facecolor=\"white\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # === Plot 2: Stepwise attention (show a subset of representative steps)\n",
        "    steps_to_plot = step_keys_sorted[:2] if len(step_keys_sorted) >= 2 else step_keys_sorted\n",
        "    for k in steps_to_plot:\n",
        "        df_step = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': step_global[k]\n",
        "        }).sort_values(by='Importance', ascending=False).head(top_k)\n",
        "\n",
        "        safe_k = str(k).replace(\"/\", \"_\").replace(\" \", \"_\")\n",
        "        fname_step = f\"tabnet_step_attention_{safe_k}_top{top_k}\"\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(data=df_step, x='Importance', y='Feature', palette=\"rocket\", edgecolor=\".2\")\n",
        "        plt.xlabel(\"Attention Score\")\n",
        "        plt.ylabel(\"\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, f\"{fname_step}.png\"), dpi=dpi, bbox_inches='tight', facecolor=\"white\")\n",
        "        plt.savefig(os.path.join(save_dir, f\"{fname_step}.pdf\"), bbox_inches='tight', facecolor=\"white\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    # --- (3) Local attention for one selected sample\n",
        "    local_sum = np.zeros(len(feature_names))\n",
        "    for k in step_keys_sorted:\n",
        "        local_sum += masks[k][sample_idx]\n",
        "    local_sum = local_sum / (len(step_keys_sorted) + 1e-12)\n",
        "\n",
        "    df_local = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': local_sum\n",
        "    }).sort_values(by='Importance', ascending=False).head(top_k)\n",
        "\n",
        "    # === Plot 3: Local attention importance\n",
        "    fname_local = f\"tabnet_local_attention_sample{sample_idx}_top{top_k}\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=df_local, x='Importance', y='Feature', palette=PALETTE_LOCAL, edgecolor=\".2\")\n",
        "    plt.xlabel(\"Attention Score\")\n",
        "    plt.ylabel(\"\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(os.path.join(save_dir, f\"{fname_local}.png\"), dpi=dpi, bbox_inches='tight', facecolor=\"white\")\n",
        "    plt.savefig(os.path.join(save_dir, f\"{fname_local}.pdf\"), bbox_inches='tight', facecolor=\"white\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return df_global, df_local, masks\n",
        "\n",
        "\n",
        "# Run TabNet attention analysis\n",
        "print(\">>> Running TabNet attention analysis...\")\n",
        "df_global_attn, df_local_attn, masks = tabnet_attention_xai_hq(\n",
        "    clf_final, X_test, feature_names, top_k=15, sample_idx=0, save_dir=\".\", dpi=600\n",
        ")\n",
        "\n",
        "# Save raw feature importances\n",
        "fi = pd.Series(clf_final.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "fi.to_csv(\"tabnet_feature_importances.csv\", header=[\"importance\"])\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "w9ssoFoenVQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. SHAP analysis\n",
        "# ==============================================================================\n",
        "print(\"\\n>>> Running SHAP analysis (this may take some time)...\")\n",
        "\n",
        "def predict_1d(X):\n",
        "    return clf_final.predict(X).reshape(-1)\n",
        "\n",
        "save_dir = \".\"\n",
        "dpi = 600\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# Sample background data for SHAP\n",
        "idx = rng.choice(len(X_train_2), size=min(500, len(X_train_2)), replace=False)\n",
        "X_bg = X_train_2[idx]\n",
        "\n",
        "masker = shap.maskers.Independent(X_bg)\n",
        "explainer = shap.PermutationExplainer(predict_1d, masker, feature_names=feature_names)\n",
        "\n",
        "# Samples to explain\n",
        "X_explain = X_test[:150]\n",
        "shap_values = explainer(X_explain, max_evals=2 * X_explain.shape[1] + 1)\n",
        "\n",
        "# --- SHAP Beeswarm Plot ---\n",
        "fname_beeswarm = \"shap_beeswarm_global_top15\"\n",
        "plt.figure(figsize=(14, 10))\n",
        "shap.plots.beeswarm(shap_values, max_display=15, show=False, color_bar_label=\"Feature Value\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(os.path.join(save_dir, f\"{fname_beeswarm}.png\"), dpi=dpi, bbox_inches='tight', facecolor=\"white\")\n",
        "plt.savefig(os.path.join(save_dir, f\"{fname_beeswarm}.pdf\"), bbox_inches='tight', facecolor=\"white\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# --- SHAP Bar Plot ---\n",
        "fname_bar = \"shap_bar_global_mean_abs_top15\"\n",
        "plt.figure(figsize=(14, 10))\n",
        "shap.plots.bar(shap_values, max_display=15, show=False)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(os.path.join(save_dir, f\"{fname_bar}.png\"), dpi=dpi, bbox_inches='tight', facecolor=\"white\")\n",
        "plt.savefig(os.path.join(save_dir, f\"{fname_bar}.pdf\"), bbox_inches='tight', facecolor=\"white\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# --- SHAP Waterfall Plot (local explanation) ---\n",
        "sample_idx = 0\n",
        "fname_waterfall = f\"shap_waterfall_local_sample{sample_idx}_top15\"\n",
        "plt.figure(figsize=(14, 8))\n",
        "shap.plots.waterfall(shap_values[sample_idx], max_display=15, show=False)\n",
        "plt.tight_layout()\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.savefig(os.path.join(save_dir, f\"{fname_waterfall}.png\"), dpi=dpi, bbox_inches='tight', facecolor=\"white\")\n",
        "fig.savefig(os.path.join(save_dir, f\"{fname_waterfall}.pdf\"), bbox_inches='tight', facecolor=\"white\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "sJzSIvU7nZZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. Permutation importance visualization\n",
        "# ==============================================================================\n",
        "save_dir = \".\"\n",
        "dpi = 600\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "FNAME = \"permutation_importance_mae_top15\"\n",
        "OUT_PNG = os.path.join(save_dir, f\"{FNAME}.png\")\n",
        "OUT_PDF = os.path.join(save_dir, f\"{FNAME}.pdf\")\n",
        "\n",
        "# Assumes df_perm is already prepared and contains:\n",
        "# columns = ['Feature', 'Importance', 'Std']\n",
        "\n",
        "try:\n",
        "    colors\n",
        "except NameError:\n",
        "    colors = sns.color_palette(\"crest\", n_colors=len(df_perm))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.barh(\n",
        "    df_perm['Feature'],\n",
        "    df_perm['Importance'],\n",
        "    xerr=df_perm['Std'],\n",
        "    color=colors,\n",
        "    edgecolor=\"0.25\",\n",
        "    capsize=4,\n",
        "    height=0.62,\n",
        "    error_kw={\"elinewidth\": 1.2, \"alpha\": 0.9}\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Mean Importance Score (MAE Degradation)\", fontsize=11)\n",
        "plt.ylabel(\"\")\n",
        "\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.grid(True, axis='x', linestyle='--', alpha=0.25, linewidth=0.8)\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "ax.spines[\"left\"].set_alpha(0.6)\n",
        "ax.spines[\"bottom\"].set_alpha(0.6)\n",
        "ax.tick_params(axis=\"y\", labelsize=10)\n",
        "ax.tick_params(axis=\"x\", labelsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(OUT_PNG, dpi=dpi, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.savefig(OUT_PDF, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(f\"Saved: {OUT_PNG} (dpi=600)\")\n",
        "print(f\"Saved: {OUT_PDF}\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import is_classifier, is_regressor\n",
        "\n",
        "print(\"is_classifier:\", is_classifier(tabnet_wrapper))\n",
        "print(\"is_regressor :\", is_regressor(tabnet_wrapper))\n",
        "\n",
        "# Check predict_proba only if available\n",
        "if hasattr(tabnet_wrapper, \"predict_proba\"):\n",
        "    p = tabnet_wrapper.predict_proba(X_test[:10])\n",
        "    print(\"predict_proba shape:\", np.asarray(p).shape)\n",
        "    print(\"proba min/max:\", np.nanmin(p), np.nanmax(p))\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SAVE_PATH_PNG = \"PDP_top_features_dpi600.png\"\n",
        "SAVE_PATH_PDF = \"PDP_top_features_dpi600.pdf\"\n",
        "\n",
        "os.makedirs(os.path.dirname(SAVE_PATH_PNG) or \".\", exist_ok=True)\n",
        "os.makedirs(os.path.dirname(SAVE_PATH_PDF) or \".\", exist_ok=True)\n",
        "\n",
        "features = list(top_idx)\n",
        "n = len(features)\n",
        "\n",
        "ncols = min(3, n)\n",
        "nrows = int(np.ceil(n / ncols))\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    nrows, ncols,\n",
        "    figsize=(5.4 * ncols, 3.6 * nrows),\n",
        "    squeeze=False\n",
        ")\n",
        "axes = axes.reshape(-1)\n",
        "\n",
        "for i, f in enumerate(features):\n",
        "    ax = axes[i]\n",
        "\n",
        "    pd = pd_one_feature(\n",
        "        estimator=tabnet_wrapper,\n",
        "        X=X_test,\n",
        "        f=f,\n",
        "        response_method=response_method,\n",
        "        class_idx=class_idx,\n",
        "        grid_resolution=80\n",
        "    )\n",
        "\n",
        "    xs = pd[\"grid_values\"][0]\n",
        "    ys = pd[\"average\"][0].reshape(-1)\n",
        "\n",
        "    m = np.isfinite(xs) & np.isfinite(ys)\n",
        "    xs, ys = xs[m], ys[m]\n",
        "\n",
        "    ax.plot(xs, ys, linewidth=2.4)\n",
        "    title = feature_names[f] if feature_names is not None else f\"feature {f}\"\n",
        "    ax.set_title(title, fontsize=12, fontweight=\"semibold\", pad=6)\n",
        "    ax.set_xlabel(\"Feature value\", fontsize=10, labelpad=4)\n",
        "    ax.set_ylabel(\"Partial dependence\", fontsize=10, labelpad=4)\n",
        "\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.25, linewidth=0.8)\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "    ax.spines[\"left\"].set_alpha(0.6)\n",
        "    ax.spines[\"bottom\"].set_alpha(0.6)\n",
        "    ax.tick_params(axis=\"both\", labelsize=9)\n",
        "\n",
        "# Remove unused axes\n",
        "for j in range(n, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "fig.subplots_adjust(\n",
        "    left=0.06, right=0.995,\n",
        "    bottom=0.14, top=0.88,\n",
        "    wspace=0.18, hspace=0.30\n",
        ")\n",
        "\n",
        "fig.savefig(SAVE_PATH_PNG, dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "fig.savefig(SAVE_PATH_PDF, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"Saved: {SAVE_PATH_PNG} (dpi=600)\")\n",
        "print(f\"Saved: {SAVE_PATH_PDF}\")\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.ticker as ticker"
      ],
      "metadata": {
        "id": "n-RV5Rx9nbpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. Group-wise residual diagnostics\n",
        "#    (high-end boxplot + strip plot)\n",
        "# ==============================================================================\n",
        "print(\"\\n>>> [Advanced Visualization] Running continent-level residual diagnostics...\")\n",
        "\n",
        "continent_col = \"Continent\"\n",
        "\n",
        "if continent_col in feature_names:\n",
        "    # 1. Prepare data\n",
        "    c_idx = feature_names.index(continent_col)\n",
        "    y_true = y_test.reshape(-1)\n",
        "    y_pred = preds.reshape(-1)\n",
        "    resid = y_true - y_pred\n",
        "\n",
        "    df_err = pd.DataFrame({\n",
        "        \"continent_code\": X_test[:, c_idx].astype(int),\n",
        "        \"abs_err\": np.abs(resid),\n",
        "        \"resid\": resid\n",
        "    })\n",
        "\n",
        "    # Stabilize log-scale visualization\n",
        "    eps = 1e-8\n",
        "    df_err[\"abs_err\"] = df_err[\"abs_err\"].clip(lower=eps)\n",
        "\n",
        "    # 2. Sort groups by median absolute error\n",
        "    sorted_idx = (\n",
        "        df_err.groupby(\"continent_code\")['abs_err']\n",
        "        .median()\n",
        "        .sort_values(ascending=False)\n",
        "        .index\n",
        "    )\n",
        "\n",
        "    # 3. Plot styling\n",
        "    plt.rcParams.update({\n",
        "        'font.family': 'sans-serif',\n",
        "        'font.size': 14,\n",
        "        'axes.titlesize': 20,\n",
        "        'axes.titleweight': 'bold',\n",
        "        'xtick.labelsize': 12,\n",
        "        'ytick.labelsize': 12\n",
        "    })\n",
        "\n",
        "    # 4. Create figure\n",
        "    fig, ax = plt.subplots(figsize=(14, 9))\n",
        "\n",
        "    # 5. Boxplot\n",
        "    sns.boxplot(\n",
        "        data=df_err,\n",
        "        x=\"continent_code\",\n",
        "        y=\"abs_err\",\n",
        "        order=sorted_idx,\n",
        "        palette=\"coolwarm_r\",\n",
        "        linewidth=2,\n",
        "        width=0.6,\n",
        "        fliersize=0,\n",
        "        ax=ax,\n",
        "        boxprops=dict(alpha=0.85)\n",
        "    )\n",
        "\n",
        "    # 6. Overlay strip plot\n",
        "    sns.stripplot(\n",
        "        data=df_err,\n",
        "        x=\"continent_code\",\n",
        "        y=\"abs_err\",\n",
        "        order=sorted_idx,\n",
        "        color=\"#2c3e50\",\n",
        "        size=3,\n",
        "        alpha=0.28,\n",
        "        jitter=0.25,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    # 7. Save file names\n",
        "    save_dir = \".\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_base = \"continent_error_analysis_highend\"\n",
        "    save_png = os.path.join(save_dir, f\"{save_base}.png\")\n",
        "    save_pdf = os.path.join(save_dir, f\"{save_base}.pdf\")\n",
        "\n",
        "    # 8. Axis labels\n",
        "    ax.set_xlabel(\"Continent Code (Sorted by Median Error)\", labelpad=12, fontweight='bold')\n",
        "    ax.set_ylabel(\"Absolute Error (Log Scale)\", labelpad=12, fontweight='bold')\n",
        "\n",
        "    # 9. Log scale\n",
        "    ax.set_yscale('log')\n",
        "    ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
        "\n",
        "    # 10. Minimal-style cleanup\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.28, color='gray', linewidth=0.8)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_linewidth(1.2)\n",
        "    ax.spines['bottom'].set_linewidth(1.2)\n",
        "\n",
        "    # 11. Add simple statistics for the top-3 highest-error groups\n",
        "    top3_codes = sorted_idx[:3]\n",
        "    for i, code in enumerate(top3_codes):\n",
        "        mean_val = df_err[df_err['continent_code'] == code]['abs_err'].mean()\n",
        "        y_pos = df_err[df_err['continent_code'] == code]['abs_err'].quantile(0.95)\n",
        "        ax.text(\n",
        "            i, y_pos,\n",
        "            f\"Mean:\\n{mean_val:.2f}\",\n",
        "            ha='center', va='bottom',\n",
        "            fontsize=11, fontweight='bold',\n",
        "            color='#c0392b'\n",
        "        )\n",
        "\n",
        "    plt.tight_layout(pad=0.6)\n",
        "\n",
        "    # 12. Save in high resolution\n",
        "    plt.savefig(save_png, dpi=600, bbox_inches='tight', facecolor='white')\n",
        "    plt.savefig(save_pdf, bbox_inches='tight', facecolor='white')\n",
        "    print(f\">>> ✅ High-resolution error diagnostic saved: {save_png}\")\n",
        "    print(f\">>> ✅ Vector PDF version saved: {save_pdf}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "else:\n",
        "    print(f\"Warning: column '{continent_col}' was not found. Skipping group-wise error analysis.\")"
      ],
      "metadata": {
        "id": "hqg-9Qw5neyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

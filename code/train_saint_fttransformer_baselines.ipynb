{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFeADBrSqESw"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# [1] Required package installation\n",
        "# ==========================================\n",
        "!pip install pytorch-tabnet pytorch-widedeep optuna\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import optuna\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# WideDeep modules\n",
        "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
        "from pytorch_widedeep.models import WideDeep, SAINT, FTTransformer\n",
        "from pytorch_widedeep import Trainer\n",
        "from pytorch_widedeep.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [1] Data loading and preprocessing\n",
        "# ==========================================\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load the harmonized dataset used for deep tabular baseline experiments.\n",
        "\n",
        "    If Dataset.csv is not found locally and the script is running in\n",
        "    Google Colab, the user is prompted to upload the file manually.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        import os\n",
        "        if not os.path.exists('Dataset.csv'):\n",
        "            print(\"=== [Google Colab] Please upload Dataset.csv ===\")\n",
        "            uploaded = files.upload()\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "        else:\n",
        "            df = pd.read_csv('Dataset.csv')\n",
        "    except:\n",
        "        df = pd.read_csv('Dataset.csv')\n",
        "    return df\n",
        "\n",
        "df = load_data()"
      ],
      "metadata": {
        "id": "p2gxSIykqYQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [1-1] Missing-value handling and basic cleanup\n",
        "# ==========================================\n",
        "if 'Country Name' in df.columns:\n",
        "    df = df.drop('Country Name', axis=1)\n",
        "\n",
        "cat_cols = ['Country Code', 'Continent']\n",
        "target_col = 'Maternal Mortality Ratio'\n",
        "\n",
        "# Missing-value treatment\n",
        "# - categorical variables: \"Unknown\"\n",
        "# - numerical variables: median imputation\n",
        "for col in cat_cols:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "num_cols = [c for c in df.columns if c not in cat_cols + ['Year', target_col]]\n",
        "for col in num_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())"
      ],
      "metadata": {
        "id": "H3cG4XsOqaGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [1-2] Temporal split design\n",
        "# ==========================================\n",
        "# Phase 1: model selection\n",
        "# Train: 2011-2014 / Validation: 2015\n",
        "train_opt = df[(df['Year'] >= 2011) & (df['Year'] <= 2014)]\n",
        "val_opt   = df[df['Year'] == 2015]\n",
        "\n",
        "# Phase 2: retraining after model selection\n",
        "# Train: 2011-2015\n",
        "train_retrain = df[(df['Year'] >= 2011) & (df['Year'] <= 2015)]\n",
        "\n",
        "# Phase 3: held-out test evaluation\n",
        "# Test: 2016\n",
        "test = df[df['Year'] == 2016]\n",
        "\n",
        "feature_names = [c for c in df.columns if c not in ['Year', target_col]]"
      ],
      "metadata": {
        "id": "eMzB-kXMqbuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [2] WideDeep preprocessing\n",
        "#     (TabPreprocessor)\n",
        "# ==========================================\n",
        "# Fit the preprocessing pipeline only on the optimization-training period\n",
        "tab_preprocessor = TabPreprocessor(\n",
        "    cat_embed_cols=cat_cols,\n",
        "    continuous_cols=num_cols,\n",
        "    scale=True\n",
        ")\n",
        "tab_preprocessor.fit(train_opt)\n",
        "\n",
        "# Transform the full dataset\n",
        "X_tab_all = tab_preprocessor.transform(df)\n",
        "y_all = df[target_col].values\n",
        "\n",
        "# Extract row indices for each temporal split\n",
        "idx_opt_train = train_opt.index\n",
        "idx_opt_val   = val_opt.index\n",
        "idx_retrain   = train_retrain.index\n",
        "idx_test      = test.index\n",
        "\n",
        "# Input parameters required by SAINT / FT-Transformer\n",
        "input_params = {\n",
        "    \"column_idx\": tab_preprocessor.column_idx,\n",
        "    \"cat_embed_input\": tab_preprocessor.cat_embed_input,\n",
        "    \"continuous_cols\": num_cols,\n",
        "}"
      ],
      "metadata": {
        "id": "eZN-VBX8qdkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [3] Unified experiment pipeline\n",
        "#     (Optuna -> Retrain -> Evaluate)\n",
        "# ==========================================\n",
        "def run_experiment(model_name):\n",
        "    \"\"\"\n",
        "    Run a complete deep tabular baseline experiment.\n",
        "\n",
        "    Workflow:\n",
        "    1. Hyperparameter optimization on 2011-2014 / 2015 split\n",
        "    2. Retraining on the full pretest period (2011-2015)\n",
        "    3. Evaluation on the held-out year 2016\n",
        "    4. Permutation-based feature importance analysis\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*10} Processing {model_name} {'='*10}\")\n",
        "\n",
        "    # --- Step 1: Hyperparameter optimization ---\n",
        "    def objective(trial):\n",
        "        # Define search space\n",
        "        if model_name == 'SAINT':\n",
        "            params = {\n",
        "                'n_blocks': trial.suggest_int('n_blocks', 1, 3),\n",
        "                'n_heads': trial.suggest_categorical('n_heads', [2, 4, 8]),\n",
        "                'attn_dropout': trial.suggest_float('attn_dropout', 0.0, 0.3),\n",
        "                'ff_dropout': trial.suggest_float('ff_dropout', 0.0, 0.3),\n",
        "                'input_dim': 64\n",
        "            }\n",
        "            tab_model = SAINT(**input_params, **params)\n",
        "\n",
        "        else:  # FT-Transformer\n",
        "            params = {\n",
        "                'n_blocks': trial.suggest_int('n_blocks', 2, 4),\n",
        "                'n_heads': trial.suggest_categorical('n_heads', [2, 4, 8]),\n",
        "                'attn_dropout': trial.suggest_float('attn_dropout', 0.0, 0.3),\n",
        "                'ff_dropout': trial.suggest_float('ff_dropout', 0.0, 0.3),\n",
        "                'input_dim': 32\n",
        "            }\n",
        "            tab_model = FTTransformer(**input_params, **params)\n",
        "\n",
        "        # Wrap the tabular model inside the WideDeep interface\n",
        "        model = WideDeep(deeptabular=tab_model)\n",
        "\n",
        "        # Train on 2011-2014 and validate on 2015\n",
        "        trainer = Trainer(model, objective=\"regression\", verbose=0)\n",
        "        trainer.fit(\n",
        "            X_train={\"X_tab\": X_tab_all[idx_opt_train], \"target\": y_all[idx_opt_train]},\n",
        "            X_val={\"X_tab\": X_tab_all[idx_opt_val], \"target\": y_all[idx_opt_val]},\n",
        "            n_epochs=15,\n",
        "            batch_size=128,\n",
        "            callbacks=[EarlyStopping(patience=5, monitor=\"val_loss\")]\n",
        "        )\n",
        "\n",
        "        return trainer.history['val_loss'][-1]\n",
        "\n",
        "    print(\">> [Step 1] Optimizing hyperparameters...\")\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    best_params = study.best_trial.params\n",
        "    print(f\"   Best parameters: {best_params}\")\n",
        "\n",
        "    # --- Step 2: Retraining on 2011-2015 ---\n",
        "    print(\">> [Step 2] Retraining on the full pretest period (2011-2015)...\")\n",
        "\n",
        "    if model_name == 'SAINT':\n",
        "        final_tab_model = SAINT(**input_params, input_dim=64, **best_params)\n",
        "    else:\n",
        "        final_tab_model = FTTransformer(**input_params, input_dim=32, **best_params)\n",
        "\n",
        "    final_model = WideDeep(deeptabular=final_tab_model)\n",
        "\n",
        "    # Train on the full pretest period without a separate validation fold\n",
        "    trainer = Trainer(final_model, objective=\"regression\", verbose=0)\n",
        "    trainer.fit(\n",
        "        X_train={\"X_tab\": X_tab_all[idx_retrain], \"target\": y_all[idx_retrain]},\n",
        "        n_epochs=30,\n",
        "        batch_size=128\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Final evaluation on the held-out year 2016 ---\n",
        "    print(\">> [Step 3] Evaluating on the held-out test year (2016)...\")\n",
        "    preds = trainer.predict(X_tab=X_tab_all[idx_test])\n",
        "    y_true = y_all[idx_test]\n",
        "\n",
        "    mae = mean_absolute_error(y_true, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
        "    r2 = r2_score(y_true, preds)\n",
        "\n",
        "    # --- Step 4: Permutation-based feature importance ---\n",
        "    print(\">> [Step 4] Calculating permutation importance...\")\n",
        "    base_score = r2_score(y_true, preds)\n",
        "    importances = {}\n",
        "\n",
        "    X_test_curr = X_tab_all[idx_test]\n",
        "    for i, col in enumerate(feature_names):\n",
        "        X_p = X_test_curr.copy()\n",
        "        np.random.shuffle(X_p[:, i])\n",
        "        preds_p = trainer.predict(X_tab=X_p)\n",
        "        importances[col] = base_score - r2_score(y_true, preds_p)\n",
        "\n",
        "    return {\n",
        "        'Metrics': {'MAE': mae, 'RMSE': rmse, 'R2': r2},\n",
        "        'Importance': pd.Series(importances).sort_values(ascending=False),\n",
        "        'Preds': preds\n",
        "    }"
      ],
      "metadata": {
        "id": "IYR7jP4-qh_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [4] Run experiments and summarize outputs\n",
        "# ==========================================\n",
        "results_saint = run_experiment('SAINT')\n",
        "results_ft = run_experiment('FT-Transformer')\n",
        "\n",
        "# --- Summary table ---\n",
        "metrics_df = pd.DataFrame({\n",
        "    'SAINT': results_saint['Metrics'],\n",
        "    'FT-Transformer': results_ft['Metrics']\n",
        "}).T[['MAE', 'RMSE', 'R2']]\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\" FINAL RESULTS (Held-out Test Year: 2016) \")\n",
        "print(\"=\"*40)\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "id": "IuOB7r_jqiWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [5] Visualization 1: performance comparison\n",
        "# ==========================================\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "metrics_df[['MAE', 'RMSE']].plot(\n",
        "    kind='bar',\n",
        "    ax=ax1,\n",
        "    width=0.4,\n",
        "    position=1,\n",
        "    color=['#00cec9', '#fab1a0']\n",
        ")\n",
        "ax1.set_ylabel(\"Error (MAE / RMSE)\", fontsize=12)\n",
        "ax1.set_title(\"Performance Comparison: SAINT vs FT-Transformer\", fontsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "metrics_df['R2'].plot(\n",
        "    kind='bar',\n",
        "    ax=ax2,\n",
        "    width=0.2,\n",
        "    position=0,\n",
        "    color='#6c5ce7',\n",
        "    label='R2 Score'\n",
        ")\n",
        "ax2.set_ylabel(\"R2 Score\", color='#6c5ce7', fontsize=12)\n",
        "ax2.set_ylim(0, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qGbHWoe3qkL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# [6] Visualization 2: permutation importance\n",
        "# ==========================================\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "results_saint['Importance'].nlargest(10).sort_values().plot(\n",
        "    kind='barh',\n",
        "    ax=axs[0],\n",
        "    color='#00cec9'\n",
        ")\n",
        "axs[0].set_title(\"SAINT Feature Importance (Permutation)\")\n",
        "\n",
        "results_ft['Importance'].nlargest(10).sort_values().plot(\n",
        "    kind='barh',\n",
        "    ax=axs[1],\n",
        "    color='#fab1a0'\n",
        ")\n",
        "axs[1].set_title(\"FT-Transformer Feature Importance (Permutation)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AjPH34gJqmMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
